# PCE RFA Model

#%%
# =============================================================================
# Import modules
# =============================================================================

# 

from matplotlib import pyplot as plt
import pandas as pd
import numpy as np
from scipy import stats
from statsmodels.tsa.api import VAR
import statsmodels.api as sm
from datetime import datetime
from datetime import timedelta
from collections import OrderedDict
from PIL import Image
import matplotlib
matplotlib.use('agg')
from scipy.optimize import differential_evolution, LinearConstraint
from scipy.optimize import minimize_scalar
pd.options.mode.chained_assignment = None  # default='warn'
import seaborn as sns
from dateutil.relativedelta import relativedelta
plt.style.use("seaborn-darkgrid")
sns.set_palette("tab10")
palette = sns.color_palette()
from tensorflow import keras
from keras.models import Sequential
from keras.layers import LSTM
from keras.layers import Dense
from keras.layers import RepeatVector
from keras.layers import TimeDistributed
import pandas as pd
import numpy as np
from dateutil.relativedelta import relativedelta
import win32com.client
import time
from datetime import date
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import ElasticNet
from sklearn.linear_model import Ridge
import statsmodels.api as sm
from sklearn.linear_model import Lasso
import time
from sklearn.decomposition import PCA
from sklearn.linear_model import LinearRegression
from sklearn.cross_decomposition import PLSRegression
from sklearn.feature_selection import RFE
from sklearn.ensemble import RandomForestRegressor
from keras import regularizers
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import GradientBoostingRegressor
pd.options.mode.chained_assignment = None  # default='warn'
from statsmodels.tsa.seasonal import seasonal_decompose
import json


def getDataframe(db, unifiedSeriesRequest):
    series = db.FetchSeries(unifiedSeriesRequest)
    return pd.DataFrame({s.Name: toPandasSeries(s) for s in series})

def toPandasSeries(series):
    # For some reason, pandas 0.19 does not like the result when to_datetime
    # is used directly on the array DatesAtStartOfPeriod. Convert to string first.
    pdates = pd.to_datetime([d.strftime('%Y-%m-%d') for d in series.DatesAtStartOfPeriod])
    return pd.Series(series.values, index=pdates)


#%%
# =============================================================================
# Loading and processing data
# =============================================================================

### Which reports are out alredy
PPI_out = True
CPI_out = True
import_prices_out = False

### Include past vintage of the PPI data?
PPI_vintage = False
vintage_date = datetime(2024, 4, 1)
# Which vintage is it, ie how many months of data are you excluding
vintage_number = 1

# Modelling exceptions:
mutual_fund_sales_pce = 'PCE: Services, Household Consumption Expenditure (for Services), Financial Services & Insurance, Financial Services, Securities Commissions, Mutual Fund Sales Charges'
mutual_fund_sales_ppi = 'PPI: Other Securities Related Services Including Margin Lending & Mutual Fund Sales'


MB_start_month = '2016-01-01'

print('Fetching Macrobond Data...')

df_input = pd.read_excel(r'N:\Common\MACRO MARKETS\Team\Forecasting\PCE\PCE_Input.xlsx', sheet_name = "PPI")

c = win32com.client.Dispatch("Macrobond.Connection")
d = c.Database
r = d.CreateUnifiedSeriesRequest()

PPI_NSA = pd.DataFrame(index = pd.date_range(MB_start_month , date.today().strftime("%Y-%m-%d"), freq= "MS"))

for i in range(len(df_input)):
    r = d.CreateUnifiedSeriesRequest()
    if PPI_vintage == True:
        
        r.AddSeries(df_input.iloc[i, 0])
        df_temp = getDataframe(d, r)
        
        s = d.FetchOneSeriesWithRevisions(df_input.iloc[i, 0])
        firstReleaseSeries = s.GetVintage(vintage_date)
        values1 = list(firstReleaseSeries.Values)
        df_temp_v = pd.Series(data=values1[:-vintage_number], index=df_temp.index[:-vintage_number], name=df_input.iloc[i, 1] )
        df_temp = df_temp_v.dropna()
        df_temp = df_temp.pct_change()*100
        df_temp = df_temp.dropna()
        result = seasonal_decompose(df_temp, model='additive', period=12)
        x = np.array(df_temp.values).flatten() - result.seasonal.values
        df_sa = pd.Series(data=x, index=df_temp.index, name=df_input.iloc[i, 1] )
        PPI_NSA = PPI_NSA.join(df_sa, how = 'left')
        
        
    else:
        r = d.CreateUnifiedSeriesRequest()
        r.AddSeries(df_input.iloc[i, 0])
        df_temp = getDataframe(d, r)
        df_temp = df_temp.pct_change() * 100
        df_temp = df_temp.dropna()
        df_temp = pd.Series(data=np.array(df_temp.values).flatten(), index=df_temp.index, name=df_input.iloc[i, 1])
        df_temp.rename(df_input.iloc[i, 1])
        PPI_NSA = PPI_NSA.join(df_temp, how='left')

if PPI_out == False:
    PPI_NSA = PPI_NSA.iloc[:-1, :]
    PPI_NSA = PPI_NSA.fillna(method = 'ffill')
PPI_NSA_ = PPI_NSA.dropna(axis=0)

print("PPI NSA Data Loaded.")


df_input = pd.read_excel(r'N:\Common\MACRO MARKETS\Team\Forecasting\PCE\PCE_Input.xlsx', sheet_name="PPI")

c = win32com.client.Dispatch("Macrobond.Connection")
d = c.Database
r = d.CreateUnifiedSeriesRequest()

PPI_SA = pd.DataFrame(index=pd.date_range(MB_start_month, date.today().strftime("%Y-%m-%d"), freq="MS"))

for i in range(len(df_input)):
    r = d.CreateUnifiedSeriesRequest()
    if PPI_vintage == True:

        r.AddSeries(df_input.iloc[i, 0])
        df_temp = getDataframe(d, r)

        s = d.FetchOneSeriesWithRevisions(df_input.iloc[i, 0])
        firstReleaseSeries = s.GetVintage(vintage_date)
        values1 = list(firstReleaseSeries.Values)
        df_temp_v = pd.Series(data=values1[:-vintage_number], index=df_temp.index[:-vintage_number],
                              name=df_input.iloc[i, 1])
        df_temp = df_temp_v.dropna()
        df_temp = df_temp.pct_change() * 100
        df_temp = df_temp.dropna()
        result = seasonal_decompose(df_temp, model='additive', period=12)
        x = np.array(df_temp.values).flatten() - result.seasonal.values
        df_sa = pd.Series(data=x, index=df_temp.index, name=df_input.iloc[i, 1])
        PPI = PPI.join(df_sa, how='left')


    else:
        r.AddSeries(df_input.iloc[i, 0])
        df_temp = getDataframe(d, r)
        df_temp = df_temp.pct_change()*100
        df_temp = df_temp.dropna()
        result = seasonal_decompose(df_temp, model='additive', period=12)
        # Check if there is a seasonal pattern
        if result.seasonal.sum() == 0:
            # If no seasonal pattern, skip this series
            continue
        x = np.array(df_temp.values).flatten() - result.seasonal.values
        df_sa = pd.Series(data=x, index=df_temp.index, name=df_input.iloc[i, 1] )
        PPI_SA = PPI_SA.join(df_sa, how = 'left')

if PPI_out == False:
    PPI_SA = PPI_SA.iloc[:-1, :]
    PPI_SA = PPI_SA.fillna(method='ffill')
PPI_SA_ = PPI_SA.dropna(axis=0)
PPI_ = PPI_NSA_.join(PPI_SA_, rsuffix = '_SA')
print("PPI SA Data Loaded.")

df_input = pd.read_excel(r'N:\Common\MACRO MARKETS\Team\Forecasting\PCE\PCE_Input.xlsx', sheet_name = "Import")

c = win32com.client.Dispatch("Macrobond.Connection")
d = c.Database
r = d.CreateUnifiedSeriesRequest()

import_prices = pd.DataFrame(index = pd.date_range(MB_start_month , date.today().strftime("%Y-%m-%d"), freq= "MS"))

for i in range(len(df_input)):
    r = d.CreateUnifiedSeriesRequest()
    r.AddSeries(df_input.iloc[i, 0])
    df_temp = getDataframe(d, r)
    df_temp = df_temp.pct_change()*100
    df_temp = df_temp.dropna()
    result = seasonal_decompose(df_temp, model='additive', period=12)
    x = np.array(df_temp.values).flatten() - result.seasonal.values
    df_sa = pd.Series(data=x, index=df_temp.index, name=df_input.iloc[i, 1] )
    import_prices = import_prices.join(df_sa, how = 'left')

if import_prices_out == False:
    import_prices = import_prices.iloc[:-1, :]
    import_prices = import_prices.fillna(method = 'ffill')
import_prices_ = import_prices.dropna()

import_prices_ = import_prices.dropna()

print("Import Price Data Loaded.")


df_input = pd.read_excel(r'N:\Common\MACRO MARKETS\Team\Forecasting\PCE\PCE_Input.xlsx', sheet_name = "CPI")

c = win32com.client.Dispatch("Macrobond.Connection")
d = c.Database
r = d.CreateUnifiedSeriesRequest()

CPI = pd.DataFrame(index = pd.date_range(MB_start_month , date.today().strftime("%Y-%m-%d"), freq= "MS"))

for i in range(len(df_input)):
    r = d.CreateUnifiedSeriesRequest()
    r.AddSeries(df_input.iloc[i, 0])
    df_temp = getDataframe(d, r)
    df_temp = df_temp.pct_change()*100
    df_temp = df_temp.dropna()
    df_temp = pd.Series(data = np.array(df_temp.values).flatten(), index=df_temp.index, name = df_input.iloc[i, 1])
    df_temp.rename(df_input.iloc[i, 1])
    CPI = CPI.join(df_temp, how = 'left')

if CPI_out == False:
    CPI = CPI.iloc[:-1, :]
    CPI = CPI.fillna(method = 'ffill')

CPI_ = CPI.dropna()

print("CPI Data Loaded.")


df_input = pd.read_excel(r'N:\Common\MACRO MARKETS\Team\Forecasting\PCE\PCE_Input.xlsx', sheet_name = "PCE_Prices")

c = win32com.client.Dispatch("Macrobond.Connection")
d = c.Database
r = d.CreateUnifiedSeriesRequest()

PCE_Prices = pd.DataFrame(index = pd.date_range(MB_start_month , date.today().strftime("%Y-%m-%d"), freq= "MS"))

for i in range(len(df_input)):
    r = d.CreateUnifiedSeriesRequest()
    r.AddSeries(df_input.iloc[i, 0])
    df_temp = getDataframe(d, r)
    df_temp = df_temp.pct_change()*100
    df_temp = df_temp.dropna()
    df_temp = pd.Series(data = np.array(df_temp.values).flatten(), index=df_temp.index, name = df_input.iloc[i, 1])
    df_temp.rename(df_input.iloc[i, 1])
    PCE_Prices = PCE_Prices.join(df_temp, how = 'left')

PCE_Prices_ = PCE_Prices.dropna()

print("PCE Prices Data Loaded.")


df_input = pd.read_excel(r'N:\Common\MACRO MARKETS\Team\Forecasting\PCE\PCE_Input.xlsx', sheet_name = "PCE_Q_P")

c = win32com.client.Dispatch("Macrobond.Connection")
d = c.Database
r = d.CreateUnifiedSeriesRequest()

PCE_Q_P = pd.DataFrame(index = pd.date_range(MB_start_month , date.today().strftime("%Y-%m-%d"), freq= "MS"))

for i in range(len(df_input)):
    r = d.CreateUnifiedSeriesRequest()
    r.AddSeries(df_input.iloc[i, 0])
    df_temp = getDataframe(d, r)
    PCE_Q_P = PCE_Q_P.join(df_temp, how = 'left')

PCE_Q_P = PCE_Q_P.dropna()


df_input = pd.read_excel(r'N:\Common\MACRO MARKETS\Team\Forecasting\PCE\PCE_Input.xlsx', sheet_name = "PCE_Q_N")

for i in range(len(df_input)):
    PCE_Q_P[df_input.iloc[i, 0]] = -PCE_Q_P[df_input.iloc[i, 0]] 

PCE_Q = PCE_Q_P.copy()

PCE_Q['Total'] = PCE_Q.sum(axis=1)
for i in PCE_Q.columns:
    PCE_Q[i] = PCE_Q[i]/PCE_Q['Total']
PCE_Q = PCE_Q.iloc[:, :-1]
PCE_Q = PCE_Q.dropna()

print("PCE Quantities Data Loaded.")
print(" ")
print("All Data Loaded.")

#%%
# =============================================================================
# Prepare the Dataframe
# =============================================================================

def rfa_regression(X, y, X_forecast_month, components, print_regression, stopping_threshold, min_r2_threshold):
    estimator = LinearRegression()
    selected_features = []
    remaining_features = list(X.columns)
    best_score = -float('inf')
    
    for _ in range(components):
        best_feature = None
        
        for feature in remaining_features:
            current_features = selected_features + [feature]
            X1 = X[current_features]
            X1['constant'] = 1
            X1 = X1.join(y, how='right')
            X1 = X1.iloc[:, :-1]
            
            reg = sm.OLS(endog=y, exog=X1, missing='drop')
            results = reg.fit()
            
            if results.rsquared_adj > best_score:
                best_score = results.rsquared_adj
                best_feature = feature
        
        selected_features.append(best_feature)
        remaining_features.remove(best_feature)
        if best_score > min_r2_threshold or best_score - results.rsquared_adj < stopping_threshold:
            break
    
    X1 = X[selected_features]
    X_forecast_month = X_forecast_month[selected_features]
    X1['constant'] = 1
    X_forecast_month['constant'] = 1
    
    X1 = X1.join(y, how='right')
    X1 = X1.iloc[:, :-1]
    
    reg = sm.OLS(endog=y, exog=X1, missing='drop')
    rfa_results = reg.fit()
    
    if print_regression:
        print(" ")
        print(rfa_results.summary2())
    
    rfa_forecast = np.dot(rfa_results.params, X_forecast_month)
    
    return rfa_forecast, rfa_results.rsquared_adj, rfa_results.params, X1.columns


PCE_Prices = PCE_Prices_.copy()
CPI = CPI_.copy()
PPI = PPI_.copy()
Imports = import_prices_.copy()

standardise = False
lags = 1
components = 4
stopping_threshold = 0.05
min_r2_threshold = 0.95

PCE_Prices_length = len(PCE_Prices.columns)
PCE_Prices = PCE_Prices.join(CPI, how = 'right')
PCE_Prices = PCE_Prices.iloc[:, :PCE_Prices_length]
PCE_Prices_lags = PCE_Prices.copy()
for i in range(lags):
    for ii in PCE_Prices_lags.columns[:PCE_Prices_length]:
        PCE_Prices_lags[ii + '_lag_' + str(i+1)] = PCE_Prices_lags[ii].shift(i+1)

PCE_Prices_lags = PCE_Prices_lags.iloc[:, PCE_Prices_length:]

df_reg = CPI.copy()

#if PPI_vintage == True:
#    df_reg = df_reg.join(PPI, how = 'right')

if PPI_vintage == False:
    df_reg = df_reg.join(PPI, how = 'left')

if import_prices_out == True:
    df_reg = df_reg.join(Imports, how = 'left')
# df_reg_length = len(df_reg.columns)
#for i in range(lags):
#    for ii in df_reg.columns[:df_reg_length]:
#        df_reg[ii + '_lag_' + str(i+1)] = df_reg[ii].shift(i+1)



df_reg = df_reg.join(PCE_Prices_lags, how = 'left')
df_reg = df_reg.iloc[lags:, :]
df_reg = df_reg.drop([df_reg.index[df_reg.index.get_loc('2020/04/01')]])
df_reg = df_reg.dropna(axis=1)


#%%
# =============================================================================
# Run the feature selection model
# =============================================================================


component_forecasts_rfa = []
component_rsq_rfa = []
rfa_params = []
rfa_regressors = []

for i in range(len(PCE_Prices.columns)):
    print("Component Number: ", i+1)
    
    indicator = PCE_Prices.columns[i]
    
    X = df_reg.copy()
    
    X_forecast_month = X.iloc[-1, :]
    X = X.iloc[:-1, :]
    
    X = X.join(PCE_Prices[indicator], how = 'inner')
    X = X.dropna()

    y = X[indicator]
    X = X.drop(indicator, axis = 1)
    
    rfa_forecast, rfa_results_rsquared_adj, rfa_param, rfa_regressor = rfa_regression(X, y, X_forecast_month, components, True, stopping_threshold, min_r2_threshold)
    if PCE_Prices.columns[i] == mutual_fund_sales_pce:
        rfa_forecast = df_reg[mutual_fund_sales_ppi][-1]
        rfa_results_rsquared_adj = 1.0
        rfa_param = [1, 0]
        rfa_regressor = [mutual_fund_sales_ppi, 'constant']
    component_forecasts_rfa.append(rfa_forecast)
    component_rsq_rfa.append(rfa_results_rsquared_adj)
    rfa_params.append(rfa_param)
    rfa_regressors.append(rfa_regressor)

print(" ")
print("Average R-Squared RFA: ", round(np.sum(component_rsq_rfa)/len(component_rsq_rfa), 3))
print("Headline PCE RFA: ", round(np.dot(component_forecasts_rfa, PCE_Q.iloc[-1, :].values), 3))

#%%
# =============================================================================
# Export results
# =============================================================================

df_groups = pd.read_excel(r'N:\Common\MACRO MARKETS\Team\Forecasting\PCE\PCE_Input.xlsx', sheet_name = "Groupings")
df_groups['Weights'] = PCE_Q.iloc[-1, :].values
df_groups['Weights_Previous'] = PCE_Q.iloc[-2, :].values
df_groups['Forecasts'] = component_forecasts_rfa  # component_forecasts_bt
df_groups['Forecasts_Previous'] = PCE_Prices.iloc[-2, :].values
df_groups['Contribution'] = df_groups['Forecasts']*df_groups['Weights']
df_groups['Contribution_Previous'] = df_groups['Forecasts_Previous']*df_groups['Weights_Previous']
df_groups['Adjusted R2'] = component_rsq_rfa

df_groups.to_excel(r'N:\Common\MACRO MARKETS\Team\Forecasting\PCE\PCE_Forecast_Output.xlsx')

grouping_list = ["Headline", "Food", "Energy", "Core", "Core Goods", "Durable Goods", "Nondurable Goods", "Core Services", "Core Services ex-Housing", "Housing", "Health Care Services", "Transportation Services", "Recreation Services", "Financial Services"]
forecast_list = []

for i in grouping_list:
    forecast_list.append(round((np.sum(df_groups[i]*df_groups['Contribution']))/(np.sum(df_groups[i]*df_groups['Weights'])), 3))


#%%
# =============================================================================
# Save params and regressors
# =============================================================================

matrix = np.full((components+1, len(PCE_Prices.columns)), np.nan)

df_params_output = pd.DataFrame(columns = PCE_Prices.columns, data = matrix)
df_regressors_output = pd.DataFrame(columns = PCE_Prices.columns, data = matrix)

for i in range(len(rfa_params)):
    for ii in range(len(rfa_params[i])):
        df_params_output.iloc[ii, i] = rfa_params[i][ii]

for i in range(len(rfa_regressors)):
    for ii in range(len(rfa_regressors[i])):
        df_regressors_output.iloc[ii, i] = rfa_regressors[i][ii]

df_regressors_output = df_regressors_output.fillna("None")

df_params_output.to_excel(r'N:\Common\MACRO MARKETS\Team\Forecasting\PCE\Params\Saved_Params.xlsx')
df_regressors_output.to_excel(r'N:\Common\MACRO MARKETS\Team\Forecasting\PCE\Params\Saved_Regressors.xlsx')

    
#%%
# =============================================================================
# Load params and regressors, backtesting and backward revisions    
# =============================================================================


df_params_input = pd.read_excel(r'N:\Common\MACRO MARKETS\Team\Forecasting\PCE\Params\Saved_Params.xlsx', index_col = 0)
df_regressors_input = pd.read_excel(r'N:\Common\MACRO MARKETS\Team\Forecasting\PCE\Params\Saved_Regressors.xlsx', index_col = 0)
df_groups = pd.read_excel(r'N:\Common\MACRO MARKETS\Team\Forecasting\PCE\PCE_Input.xlsx', sheet_name = "Groupings")


df_quick = df_reg.copy()
df_quick['constant'] = 1

# Set i as how many months ago you want to check regression result, i = 1 is the most recent release
i = 0

component_forecasts_quick = []

for ii in range(len(PCE_Prices.columns)):
    count = 0.0
    for iii in range(len(df_params_input[PCE_Prices.columns[ii]])):
        if type(df_regressors_input.iloc[iii, ii]) == str:
            count += df_quick[df_regressors_input.iloc[iii, ii]][-i-1]*df_params_input.iloc[iii, ii]
    component_forecasts_quick.append(count)

print(" ")
print("Headline PCE BT: ", round(np.dot(component_forecasts_quick, PCE_Q.iloc[-i-1, :].values), 3))

print(" ")
print("Core PCE BT: ", round(np.dot(component_forecasts_quick, PCE_Q.iloc[-i-1, :].values*df_groups['Core'])/np.dot(PCE_Q.iloc[-i-1, :].values,df_groups['Core']), 3))


print(" ")
print("Core Services ex Housing PCE BT: ", round(np.dot(component_forecasts_quick, PCE_Q.iloc[-i-1, :].values*df_groups['Core Services ex-Housing'])/np.dot(PCE_Q.iloc[-i-1, :].values,df_groups['Core Services ex-Housing']), 3))



#%%
# =============================================================================
# Backtesting
# =============================================================================


df_bt = df_reg.copy()
df_bt['constant'] = 1

# Set i as how many months ago you want to check regression result, i = 1 is the most recent release
i = 0
component_forecasts_bt = []

for ii in range(len(rfa_params)):
    count = 0.0
    for iii in range(len(rfa_params[ii])):
        count += df_bt[rfa_regressors[ii][iii]][-i-1]*rfa_params[ii][iii]
    component_forecasts_bt.append(count)

print(" ")
print("Headline PCE BT: ", round(np.dot(component_forecasts_bt, PCE_Q.iloc[-i-1, :].values), 3))

print(" ")
print("Core PCE BT: ", round(np.dot(component_forecasts_bt, PCE_Q.iloc[-i-1, :].values*df_groups['Core'])/np.dot(PCE_Q.iloc[-i-1, :].values,df_groups['Core']), 3))


df_output = pd.DataFrame(index = PCE_Prices.columns)
df_output['Actual'] = PCE_Prices.iloc[-i-1, :].values
df_output['Components'] = component_forecasts_bt
df_output['Weights'] = PCE_Q.iloc[-i-1, :].values
df_output['Adj-R2'] = component_rsq_rfa
df_output.to_excel(r'N:\Common\MACRO MARKETS\Models\Models\Stuart\CPI_output_test2.xlsx')

PPI_.to_excel(r'N:\Common\MACRO MARKETS\Models\Models\Stuart\CPI_output_test3.xlsx')


# df_rfa_params = pd.DataFrame(rfa_params)
# to_excel(r'N:\Common\MACRO MARKETS\Models\Models\Stuart\PCE_params_output.xlsx')
# rfa_regressors







































# RFA Forecasting main

#%%

# =============================================================================
# 1 - Import modules
# =============================================================================

# pkg_resources.get_distribution("pandas").version

def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn

import sys
import os
import numpy.linalg as la
import dateutil.relativedelta
from datetime import datetime
import itertools
import warnings
from ast import literal_eval as make_tuple
import win32com.client
import math
import matplotlib
matplotlib.use('agg')
import matplotlib.pyplot as plt
from macrobond_api_constants import SeriesToHigherFrequencyMethod as h
from macrobond_api_constants import SeriesToLowerFrequencyMethod as l
from macrobond_api_constants import SeriesPartialPeriodsMethod as pp
from macrobond_api_constants import SeriesFrequency as f
from macrobond_api_constants import SeriesWeekdays as wd
import Tcl
from PIL import Image
import pandas as pd
import numpy as np
from dateutil.relativedelta import relativedelta
import win32com.client
import time
from datetime import date
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import ElasticNet
from sklearn.linear_model import Ridge
import statsmodels.api as sm
from sklearn.linear_model import Lasso
import time
from sklearn.decomposition import PCA
from sklearn.linear_model import LinearRegression
from sklearn.cross_decomposition import PLSRegression
from sklearn.feature_selection import RFE
from sklearn.ensemble import RandomForestRegressor
from keras import regularizers
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.svm import LinearSVR
from sklearn.neighbors import KNeighborsRegressor
pd.options.mode.chained_assignment = None  # default='warn'
from statsmodels.tsa.stattools import acf
from statsmodels.tsa.stattools import adfuller
import matplotlib.patches as mpatches
from matplotlib.legend_handler import HandlerTuple
from sklearn.ensemble import RandomForestRegressor
from statsmodels.tsa.arima.model import ARIMA
from sklearn.neural_network import MLPRegressor
from statsmodels.tsa.statespace.sarimax import SARIMAX

def getDataframe(db, unifiedSeriesRequest):
    series = db.FetchSeries(unifiedSeriesRequest)
    return pd.DataFrame({s.Name: toPandasSeries(s) for s in series})

def toPandasSeries(series):
    # For some reason, pandas 0.19 does not like the result when to_datetime
    # is used directly on the array DatesAtStartOfPeriod. Convert to string first.
    pdates = pd.to_datetime([d.strftime('%Y-%m-%d') for d in series.DatesAtStartOfPeriod])
    return pd.Series(series.values, index=pdates)



#%%
# =============================================================================
# 2 - Pull data
# Specify Input Sheet (Main, PPI, Prices, Main ex PMIs, Surveys, Spare, US_Labour, US_Labour_NSA) - if not using Main, bracket out rows 129-133
# =============================================================================

MB_start_month = '2010-01-01'

# frequency of target indicator: MS/ QS
freq = 'MS' # QS: quarterly

print('Fetching Macrobond Data...')
df_input = pd.read_excel(r'N:\Common\MACRO MARKETS\Team\Forecasting\Data_Input\input_data.xlsx', sheet_name = 'Main')

#monthly date formatting: 01/mm/yyyy
df_manually_added_variables = pd.read_excel(r'N:\Common\MACRO MARKETS\Team\Forecasting\Data_Input\manual_input_variables.xlsx')

df_dict = df_input.copy()

c = win32com.client.Dispatch("Macrobond.Connection")
d = c.Database

tickers_list = df_input['Tickers']
names_list = df_input['Names']
transform_list = df_input['Transform']

r = d.CreateUnifiedSeriesRequest()
df1 = pd.DataFrame(index = pd.date_range(MB_start_month , date.today().strftime("%Y-%m-%d"), freq= freq))

for i in range(len(tickers_list)):
    if df_input['Remove'][i] == 0:
        start = time.time()
        r = d.CreateUnifiedSeriesRequest()
        r.AddSeries(tickers_list[i])
        df_temp = getDataframe(d, r)
        df_temp = df_temp.resample(freq).mean()
        if transform_list[i] == 'diff':
            df_temp = df_temp.diff()
        if transform_list[i] == 'log':
            df_temp = df_temp.pct_change()*100
        if transform_list[i] == 'yoy':
            df_temp = df_temp.pct_change(periods=12)*100
        if transform_list[i] == '%ewma':
            df_temp = df_temp.pct_change().multiply(100).ewm(alpha=0.2,min_periods=12)
        if transform_list[i] == 'ewma_Q':
            df_temp = df_temp.pct_change().multiply(100).ewm(alpha=0.5,min_periods=3)
        df1 = df1.join(df_temp, how = 'left')
        end = time.time()
        elapsed_time = end - start
        print('Number: ', i+1, ', Ticker: ', tickers_list[i], ', Loop time:', round(elapsed_time, 2), 'seconds')

df1 = df1.join(df_manually_added_variables, how = 'left')

#df1['ISM_M_New_Orders'] = df1['ussurv0001'] + df1['ussurv0002']/2
#df1['ISM_M_Production'] = df1['ussurv0005'] + df1['ussurv0006']/2
#df1['ISM_M_Employment'] = df1['ussurv0009'] + df1['ussurv0010']/2
#df1['ISM_M_Deliveries'] = df1['ussurv0013'] + df1['ussurv0014']/2
#df1['ISM_M_Inventories'] = df1['ussurv0017'] + df1['ussurv0018']/2

#df1['ISM_S_Activity'] = df1['ussurv1601'] + df1['ussurv1602']/2
#df1['ISM_S_Orders'] = df1['ussurv1604'] + df1['ussurv1605']/2
#df1['ISM_S_Employment'] = df1['ussurv1607'] + df1['ussurv1608']/2
#df1['ISM_S_Deliveries'] = df1['ussurv1610'] + df1['ussurv1611']/2
#df1['ISM_S_Prices'] = df1['ussurv1617'] + df1['ussurv1618']/2


print("Data Loaded.")


#%%
# =============================================================================
# 3 - Parameters and functions
# =============================================================================

# Indicator of choice, some common ones: NFP SA initial (uslama4979) NFP SA revised (uslama1060), AHE(uslama0001) sp500_500 (S&P), Control Retail Sales (ustrad5000), Core CPI (uspric2373), Headline (uspric2156)
# Empire (ussurv1000), Philly (ussurv1102), ISM Manuf (ussurv1055), ISM M Prices (ussurv1062) ISM S (ussurv1044), ISM S Prices (ussurv1050) PPI Core (uspric6205), Kansas (ussurv0995)
# PMI Manuf (markit_pmiusmanmepm), PMI Services (markit_pmiussermeob), Chicago MNI (ussurv1363), Dallas (ussurv1282), Richmond (ussurv1066)
# Personal income (usnaac3000), PCE (usnaac33301), US IP (usprod1022)
# GDP (usnaac0169), PCE (usnaac0170), PCE Goods (usnaac0171), PCE Services (usnaac0182), GDP investment (usnaac0191), US Net Exports (usnaac0206), US (usnaac0213)
# ECI (uslama4626), Jolts (uslama1304), ADP (uslama9924), UMich Sentiment (ussurv0354), Core Durable Goods Orders (usprod1425)
# Challenger (uslama4502), US Durable Goods ex Transport (usprod1429)
# ZEW expectations (desurv0017), ZEW current situation (desurv0018), IFO expectations (desurv01048), IFO current situation (desurv01047)
# France PMIs: Manufacturing: (markit_pmifrmanpm), Services (markit_pmifrserob), Manufacturing Output (markit_pmifrmanob)
# German PMIs: Manufacturing: (markit_pmidemanpm), Services (markit_pmideserob), Manufacturing Output (markit_pmidemanob)
# Euro area PMIs: Manufacturing: (markit_pmiezmanpm), Services (markit_pmiezserob), Manufacturing Output (markit_pmiezmanob)
# UK PMIs: Manufacturing: (markit_pmigbmanpm), Services (markit_pmigbserob), Manufacturing Output (markit_pmigbmanob)
# US New Home Sales (uscons1171), Housing starts (uscons1156), Existing Home Sales (uscons2574), Housing Permits (uscons1146)
# uspric6205 (Core PPI), bls_wpsfd411 (Food PPI) bls_wpsfd412 (Energy PPI)
# Compostite PMIs: EZ (markit_pmiezcomob), UK (markit_pmigbcomob), US(markit_pmiuscommeob)
# ISM M New Orders (ISM_M_New_Orders) # ISM M Production (ISM_M_Production) # ISM M Employment (ISM_M_Employment)
# ISM M Deliveries (ISM_M_Deliveries) # ISM M Inventories (ISM_M_Inventories)


# in_differences = True 

def rfa_linear_regression(X, y, X_forecast_month, components, print_regression, backtest_obs, differenced, stationarity_check, directional_adjustment):
    estimator = LinearRegression()
    selected_features = []
    remaining_features = list(X.columns)
    
    for _ in range(components):
        best_score = -float('inf')
        best_feature = None
        
        for feature in remaining_features:
            current_features = selected_features + [feature]
            X1 = X[current_features]
            X1['constant'] = 1
            X1 = X1.join(y, how='right')
            X1 = X1.iloc[:, :-1]
            
            reg = sm.OLS(endog=y, exog=X1, missing='drop')
            results = reg.fit()
            
            if results.rsquared_adj > best_score:
                best_score = results.rsquared_adj
                best_feature = feature
        
        selected_features.append(best_feature)
        remaining_features.remove(best_feature)
    
    X1 = X[selected_features]
    X_forecast_month = X_forecast_month[selected_features]
    X1['constant'] = 1
    X_forecast_month['constant'] = 1
    
    X1 = X1.join(y, how='right')
    X1 = X1.iloc[:, :-1]
    
    reg = sm.OLS(endog=y, exog=X1, missing='drop')
    rfa_results = reg.fit()
    
    if print_regression:
        print(" ")
        print(rfa_results.summary2())
    
    rfa_forecast = np.dot(rfa_results.params, X_forecast_month)
    
    reg_bt = sm.OLS(endog=y[:-backtest_obs], exog=X1.iloc[:-backtest_obs, :], missing='drop')
    rfa_results_bt = reg_bt.fit()
       
    mae_bt_list = []
    forecast_misses = []
    
    for i in range(backtest_obs):
        reg_bt = sm.OLS(endog=y[:-backtest_obs+i], exog=X1.iloc[:-backtest_obs+i, :], missing='drop')
        rfa_results_bt = reg_bt.fit()
        rfa_forecast_bt = np.dot(rfa_results_bt.params, X1.iloc[-i-1, :])
        mae_bt_list.append(np.abs(rfa_forecast_bt-y[-i-1]))
        forecast_misses.append(rfa_forecast_bt-y[-i-1])
        
    print(" ")
    print("Forecast Misses: ", forecast_misses)
        
    rfa_mae = np.mean(mae_bt_list)
    
    rfa_average_miss = np.mean(forecast_misses)
    
    if directional_adjustment:
        rfa_forecast = rfa_forecast - rfa_average_miss
    
    if differenced:
        rfa_forecast = rfa_forecast + stationarity_check[stationarity_check.last_valid_index()]
    
    print(" ")
    print("RFA: Linear Regression")
    print("Number of regressors: ", components)
    print('Adj R-Squared: ', rfa_results.rsquared_adj)
    print('Forecast: ', round(rfa_forecast, 2))
    print('MAE: ', round(rfa_mae, 2))
    print('Prior ', stationarity_check[stationarity_check.last_valid_index()])

    return rfa_forecast, rfa_results.rsquared_adj, rfa_mae

def rfa_linear_regression_mae(X, y, X_forecast_month, components, print_regression, backtest_obs, differenced, stationarity_check):
    estimator = LinearRegression()
    selected_features = []
    remaining_features = list(X.columns)
    
    for _ in range(components):
        lowest_mae = float('inf')
        best_feature = None
        
        for feature in remaining_features:
            current_features = selected_features + [feature]
            X1 = X[current_features]
            X1['constant'] = 1
            X1 = X1.join(y, how='right')
            X1 = X1.iloc[:, :-1]
            
            reg = sm.OLS(endog=y, exog=X1, missing='drop')
            results = reg.fit()
                    
            for m in range(len(results.params)):
                X1[X1.columns[m]] = X1[X1.columns[m]]*results.params[m]
            X1['forecast'] = X1.sum(axis=1)
           
            mae_bt_test_list = []
            for mm in range(backtest_obs*10):
                mae_bt_test_list.append(abs(X1['forecast'][-backtest_obs-1-mm] - y[-backtest_obs-1-mm]))
           
            test_mae = np.average(mae_bt_test_list)
                    
            if test_mae < lowest_mae:
                lowest_mae = test_mae
                best_feature = feature
        
        selected_features.append(best_feature)
        remaining_features.remove(best_feature)
    
    X1 = X[selected_features]
    X_forecast_month = X_forecast_month[selected_features]
    X1['constant'] = 1
    X_forecast_month['constant'] = 1
    
    X1 = X1.join(y, how='right')
    X1 = X1.iloc[:, :-1]
    
    reg = sm.OLS(endog=y, exog=X1, missing='drop')
    rfa_results = reg.fit()
    
    if print_regression:
        print(" ")
        print(rfa_results.summary2())
    
    rfa_forecast = np.dot(rfa_results.params, X_forecast_month)
    
    reg_bt = sm.OLS(endog=y[:-backtest_obs], exog=X1.iloc[:-backtest_obs, :], missing='drop')
    rfa_results_bt = reg_bt.fit()
       
    mae_bt_list = []
    
    for i in range(backtest_obs):
        reg_bt = sm.OLS(endog=y[:-backtest_obs+i], exog=X1.iloc[:-backtest_obs+i, :], missing='drop')
        rfa_results_bt = reg_bt.fit()
        rfa_forecast_bt = np.dot(rfa_results_bt.params, X1.iloc[-i-1, :])
        mae_bt_list.append(np.abs(rfa_forecast_bt-y[-i-1]))
        
    rfa_mae = np.mean(mae_bt_list)
    
    if differenced == True:
        rfa_forecast = rfa_forecast + stationarity_check[stationarity_check.index.get_loc(forecast_month)-1]
    
    print(" ")
    print("RFA: Linear Regression")
    print("Number of regressors: ", components)
    print('Adj R-Squared: ', rfa_results.rsquared_adj)
    print('Forecast: ', round(rfa_forecast, 2))
    print('MAE: ', round(rfa_mae, 2))
    print('Prior ', stationarity_check[stationarity_check.index.get_loc(forecast_month)-1])

    return rfa_forecast, rfa_results.rsquared_adj, rfa_mae

def adf_test(df):
    adf_results = adfuller(df.dropna(how='any').values[:,0], regression='c', autolag='AIC', regresults=True)
    if adf_results[0] <= adf_results[2]['1%']:
        #print(df.columns[0] + ' is already stationary.')
        df_int = 0
    elif adf_results[0] > adf_results[2]['1%']:
        df = df.diff(periods=1)
        adf_results = adfuller(df.dropna(how='any').values[:,0], regression='n', autolag='AIC', regresults=True)
        if adf_results[0] <= adf_results[2]['1%']:
            #print(df.columns[0] + ' is I(1) and has been first-differenced.')
            df_int = 1
        elif adf_results[0] > adf_results[2]['1%']:
            df = df.diff(periods=1)
            adf_results = adfuller(df.dropna(how='any').values[:, 0], regression = 'n', autolag = 'AIC', regresults = True)
            if adf_results[0] <= adf_results[2]['1%']:
                #print(df.columns[0] + ' is I(2) and has been twice-differenced.')
                df_int = 2
            elif adf_results[0] > adf_results[2]['1%']:
                #print(df.columns[0] + ' is more than I(2); check underlying data.')
                df = pd.DataFrame(np.nan, index=df.index, columns=df.columns)

    return df, df_int

def make_stationary(time_series, max_diff=1, significance_level=0.2):
    """
    Make a time series stationary by iteratively differencing.

    Parameters:
        time_series (list or numpy array): The input time series.
        max_diff (int): The maximum number of differencing iterations allowed.
        significance_level (float): The significance level for the ADF test.

    Returns:
        list: The stationary time series after differencing.
    """
    diff_count = 0
    while not is_stationary(time_series, significance_level) and diff_count < max_diff:
        time_series = np.diff(time_series)
        diff_count += 1
        
        
    for i in range(diff_count):
        time_series = np.insert(time_series, 0, 0)
        
    return time_series

def is_stationary(time_series, significance_level=0.2):
    """
    Check if a time series is stationary using the Augmented Dickey-Fuller (ADF) test.

    Parameters:
        time_series (list or numpy array): The input time series.
        significance_level (float): The significance level for the ADF test.

    Returns:
        bool: True if stationary, False otherwise.
    """
    result = adfuller(time_series)
    p_value = result[1]

    return p_value < significance_level

# =============================================================================
# Stationary
# =============================================================================

indicator = "ussurv1044" # Which indicator you want to forecast
forecast_month = '2024-06-01' # What's the reference month of the release you're forecasting, yyyy-mm-01
lags = 2 # How many lags of the input data you want to consider
regressand_12m_lag = False # Inlcude the same release from last year as a potential regressor
components = 2 # Features selected minimum, maximum will be this plus +12
decimals = 2 # Number of decimals you want in the reported forecast
backtest_obs = 12 # Test sample size no of months
remove_covid = True
stationary = True # do not change
multiple = 5 # obsolete
directional_adjustment = False  # residual adj for avg error in backtest ob sample period
trim_y = False # outlier removal for target var
six_mma = True # to add a 6mma of target var

df2 = df1.copy()

end_date = forecast_month
last_date = df2.index.max() + pd.DateOffset(months=1)
date_range = pd.date_range(start=last_date, end=end_date, freq='MS')
new_dates_df = pd.DataFrame(index=date_range)
df2 = pd.concat([df2, new_dates_df])


original_length = len(df2.columns)

stationarity_check = df2[indicator]

if lags != 0:
    for i in range(lags):
        for ii in df2.columns[:original_length]:
            df2[ii + '_lag_' + str(i+1)] = df2[ii].shift(i+1)

if regressand_12m_lag:
    df2[indicator + "_lag_12"] = df2[indicator].shift(12)

if six_mma:
    df2[indicator + "_6mma"] = df2[indicator].rolling(window=6).mean()

df2 = df2.fillna(method = 'bfill')  # imputes by backfill

# Make data stationary
if stationary == True:
    for xx in df2.columns:
        try:
            df2[xx] = make_stationary(df2[xx])
        except:
            pass
# Make stationary as stated above
stationarity_check_ = df2[indicator]
if stationarity_check.values[stationarity_check.index.get_loc(forecast_month)-1] == stationarity_check_.values[stationarity_check.index.get_loc(forecast_month)-1]:
    differenced = False
else:
    differenced = True

# list of dates to remove from  dataset
if remove_covid == True:
    
    if freq == 'QS':
        dates_to_remove = ["2020-01-01", "2020-04-01", "2020-07-01"]
    
    if freq == 'MS':
        dates_to_remove = ["2020-03-01", "2020-04-01", "2020-05-01", "2020-06-01", "2020-07-01", "2020-08-01"]


for i in dates_to_remove:
    df2 = df2.drop([df2.index[df2.index.get_loc(i)]])


if trim_y:
    lower_bound = df2[indicator].quantile(0.1)  # Lower 10th percentile
    upper_bound = df2[indicator].quantile(0.9)  # Upper 90th percentile
    
    # Filter the DataFrame to remove outliers
    df3 = df2[(df2[indicator].isna()) | ((df2[indicator] >= lower_bound) & (df2[indicator] <= upper_bound))]


y = df2[indicator]
X = df2.drop(indicator, axis=1)

to_keep = []
X_latest = X.iloc[X.index.get_loc(forecast_month), :].notnull()
for i in range(len(X_latest)):
    if X_latest[i] == True:
        to_keep.append(X_latest.index[i])

X = X[to_keep]

X_forecast_month = X.iloc[X.index.get_loc(forecast_month), :]
X_pcr = X.iloc[:X.index.get_loc(forecast_month)+1, :]
X = X.iloc[:X.index.get_loc(forecast_month), :]

y = y.dropna()
X = y.to_frame().join(X, how = 'left')

# PCA regression
X_pcr = X_pcr.fillna(method = 'ffill')
X_pcr = X_pcr.fillna(method = 'bfill')

X = X.fillna(method = 'ffill')
X = X.fillna(method = 'bfill')
X = X.drop(indicator, axis=1)

print("Iteration 1")
rfa_linear_forecast1, rfa__linear_results_rsquared_adj1, rfa_linear_mae1 = rfa_linear_regression(X, y, X_forecast_month, components, True, backtest_obs, differenced, stationarity_check, directional_adjustment)
print("Iteration 2")
rfa_linear_forecast2, rfa__linear_results_rsquared_adj2, rfa_linear_mae2 = rfa_linear_regression(X, y, X_forecast_month, components+2, False, backtest_obs, differenced, stationarity_check, directional_adjustment)
print("Iteration 3")
rfa_linear_forecast3, rfa__linear_results_rsquared_adj3, rfa_linear_mae3 = rfa_linear_regression(X, y, X_forecast_month, components+4, False, backtest_obs, differenced, stationarity_check, directional_adjustment)
print("Iteration 4")
rfa_linear_forecast4, rfa__linear_results_rsquared_adj4, rfa_linear_mae4 = rfa_linear_regression(X, y, X_forecast_month, components+6, False, backtest_obs, differenced, stationarity_check, directional_adjustment)
print("Iteration 5")
rfa_linear_forecast5, rfa__linear_results_rsquared_adj5, rfa_linear_mae5 = rfa_linear_regression(X, y, X_forecast_month, components+8, False, backtest_obs, differenced, stationarity_check, directional_adjustment)
print("Iteration 6")
rfa_linear_forecast6, rfa__linear_results_rsquared_adj6, rfa_linear_mae6 = rfa_linear_regression(X, y, X_forecast_month, components+10, False, backtest_obs, differenced, stationarity_check, directional_adjustment)
print("Iteration 7")
rfa_linear_forecast7, rfa__linear_results_rsquared_adj7, rfa_linear_mae7 = rfa_linear_regression(X, y, X_forecast_month, components+12, True, backtest_obs, differenced, stationarity_check, directional_adjustment)


# =============================================================================
# No Stationarity
# =============================================================================

stationary = False # -> don't first difference the data

df2 = df1.copy()

end_date = forecast_month
last_date = df2.index.max() + pd.DateOffset(months=1)
date_range = pd.date_range(start=last_date, end=end_date, freq='MS')
new_dates_df = pd.DataFrame(index=date_range)
df2 = pd.concat([df2, new_dates_df])

original_length = len(df2.columns)

stationarity_check = df2[indicator]

if lags != 0:
    for i in range(lags):
        for ii in df2.columns[:original_length]:
            df2[ii + '_lag_' + str(i+1)] = df2[ii].shift(i+1)

if regressand_12m_lag:
    df2[indicator + "_lag_12"] = df2[indicator].shift(12)

if six_mma:
    df2[indicator + "_6mma"] = df2[indicator].rolling(window=6).mean()


df2 = df2.fillna(method = 'bfill')

# Make data stationary
if stationary == True:
    for xx in df2.columns:1
        try:
            df2[xx] = make_stationary(df2[xx])
        except:
            pass

stationarity_check_ = df2[indicator]
if stationarity_check.values[stationarity_check.index.get_loc(forecast_month)-1] == stationarity_check_.values[stationarity_check.index.get_loc(forecast_month)-1]:
    differenced = False
else:
    differenced = True

# list of dates to remove from  dataset
if remove_covid == True:
    
    if freq == 'QS':
        dates_to_remove = ["2020-01-01", "2020-04-01", "2020-07-01"]
    
    if freq == 'MS':
        dates_to_remove = ["2020-03-01", "2020-04-01", "2020-05-01", "2020-06-01", "2020-07-01", "2020-08-01"]

    
for i in dates_to_remove:
    df2 = df2.drop([df2.index[df2.index.get_loc(i)]])


if trim_y:
    lower_bound = df2[indicator][:-1].quantile(0.1)  # Lower 10th percentile
    upper_bound = df2[indicator][:-1].quantile(0.9)  # Upper 90th percentile
    
    # Filter the DataFrame to remove outliers
    df3 = df2[(df2[indicator].isna()) | ((df2[indicator] >= lower_bound) & (df2[indicator] <= upper_bound))]



y = df2[indicator]
X = df2.drop(indicator, axis=1)

to_keep = []
X_latest = X.iloc[X.index.get_loc(forecast_month), :].notnull()
for i in range(len(X_latest)):
    if X_latest[i] == True:
        to_keep.append(X_latest.index[i])

X = X[to_keep]

X_forecast_month = X.iloc[X.index.get_loc(forecast_month), :]
X_pcr = X.iloc[:X.index.get_loc(forecast_month)+1, :]
X = X.iloc[:X.index.get_loc(forecast_month), :]

y = y.dropna()
X = y.to_frame().join(X, how = 'left')

X_pcr = X_pcr.fillna(method = 'ffill')
X_pcr = X_pcr.fillna(method = 'bfill')

X = X.fillna(method = 'ffill')
X = X.fillna(method = 'bfill')
X = X.drop(indicator, axis=1)


print("Iteration 8")
rfa_linear_forecast8, rfa__linear_results_rsquared_adj8, rfa_linear_mae8 = rfa_linear_regression(X, y, X_forecast_month, components, False, backtest_obs, differenced, stationarity_check, directional_adjustment)
print("Iteration 9")
rfa_linear_forecast9, rfa__linear_results_rsquared_adj9, rfa_linear_mae9 = rfa_linear_regression(X, y, X_forecast_month, components+2, False, backtest_obs, differenced, stationarity_check, directional_adjustment)
print("Iteration 10")
rfa_linear_forecast10, rfa__linear_results_rsquared_adj10, rfa_linear_mae10 = rfa_linear_regression(X, y, X_forecast_month, components+4, False, backtest_obs, differenced, stationarity_check, directional_adjustment)
print("Iteration 11")
rfa_linear_forecast11, rfa__linear_results_rsquared_adj11, rfa_linear_mae11 = rfa_linear_regression(X, y, X_forecast_month, components+6, False, backtest_obs, differenced, stationarity_check, directional_adjustment)
print("Iteration 12")
rfa_linear_forecast12, rfa__linear_results_rsquared_adj12, rfa_linear_mae12 = rfa_linear_regression(X, y, X_forecast_month, components+8, False, backtest_obs, differenced, stationarity_check, directional_adjustment)
print("Iteration 13")
rfa_linear_forecast13, rfa__linear_results_rsquared_adj13, rfa_linear_mae13 = rfa_linear_regression(X, y, X_forecast_month, components+10, False, backtest_obs, differenced, stationarity_check, directional_adjustment)
print("Iteration 14")
rfa_linear_forecast14, rfa__linear_results_rsquared_adj14, rfa_linear_mae14 = rfa_linear_regression(X, y, X_forecast_month, components+12, True, backtest_obs, differenced, stationarity_check, directional_adjustment)


#%%
# =============================================================================
# 4 -  Print results
# lists forecasts and MAEs for each version of each model
# =============================================================================

print(" ")
print("Prior: ", str(stationarity_check[stationarity_check.index.get_loc(forecast_month)-1]))
print(" ")
print("RFA Linear 1: ", round(rfa_linear_forecast1, decimals), "MAE: ", round(rfa_linear_mae1, decimals))
print("RFA Linear 2: ", round(rfa_linear_forecast2, decimals), "MAE: ", round(rfa_linear_mae2, decimals))
print("RFA Linear 3: ", round(rfa_linear_forecast3, decimals), "MAE: ", round(rfa_linear_mae3, decimals))
print("RFA Linear 4: ", round(rfa_linear_forecast4, decimals), "MAE: ", round(rfa_linear_mae4, decimals))
print("RFA Linear 5: ", round(rfa_linear_forecast5, decimals), "MAE: ", round(rfa_linear_mae5, decimals))
print("RFA Linear 6: ", round(rfa_linear_forecast6, decimals), "MAE: ", round(rfa_linear_mae6, decimals))
print("RFA Linear 7: ", round(rfa_linear_forecast7, decimals), "MAE: ", round(rfa_linear_mae7, decimals))
print("RFA Linear 8: ", round(rfa_linear_forecast8, decimals), "MAE: ", round(rfa_linear_mae8, decimals))
print("RFA Linear 9: ", round(rfa_linear_forecast9, decimals), "MAE: ", round(rfa_linear_mae9, decimals))
print("RFA Linear 10: ", round(rfa_linear_forecast10, decimals), "MAE: ", round(rfa_linear_mae10, decimals))
print("RFA Linear 11: ", round(rfa_linear_forecast11, decimals), "MAE: ", round(rfa_linear_mae11, decimals))
print("RFA Linear 12: ", round(rfa_linear_forecast12, decimals), "MAE: ", round(rfa_linear_mae12, decimals))
print("RFA Linear 13: ", round(rfa_linear_forecast13, decimals), "MAE: ", round(rfa_linear_mae13, decimals))
print("RFA Linear 14: ", round(rfa_linear_forecast14, decimals), "MAE: ", round(rfa_linear_mae14, decimals))


mae_list = [rfa_linear_mae1,
            rfa_linear_mae2,
            rfa_linear_mae3,
            rfa_linear_mae4,
            rfa_linear_mae5,
            rfa_linear_mae6,
            rfa_linear_mae7,
            rfa_linear_mae8,
            rfa_linear_mae9,
            rfa_linear_mae10,
            rfa_linear_mae11,
            rfa_linear_mae12,
            rfa_linear_mae13,
            rfa_linear_mae14]

forecast_list = [rfa_linear_forecast1,
                 rfa_linear_forecast2,
                 rfa_linear_forecast3,
                 rfa_linear_forecast4,
                 rfa_linear_forecast5,
                 rfa_linear_forecast6,
                 rfa_linear_forecast7,
                 rfa_linear_forecast8,
                 rfa_linear_forecast9,
                 rfa_linear_forecast10,
                 rfa_linear_forecast11,
                 rfa_linear_forecast12,
                 rfa_linear_forecast13,
                 rfa_linear_forecast14] 


# Only in levels: 
#mae_list = mae_list[8:]
#forecast_list = forecast_list[8:]

# Only in changes: 
#mae_list = mae_list[:8]
#forecast_list = forecast_list[:8]


# Drop 'n' of the least accurate models
n_drop = 0
for i in range(n_drop):
    max_value = max(mae_list)
    index_to_drop = mae_list.index(max_value)
    del mae_list[index_to_drop]
    del forecast_list[index_to_drop]

print(" ")
print('Average MAE among selected: ', np.mean(mae_list))


mae_list = [1/i**2 for i in mae_list]
mae_list = mae_list/np.sum(mae_list)


# Manual adjustment: manually adjust the number if you have ad hoc analysis to add (eg NFP weather effects, Seasonal Factor bias)
manual_adjustment = 0
forecast_list = [i +manual_adjustment for i in forecast_list]

if indicator == 'uslama4979' or indicator == 'uslama1304':
    forecast_list = [i/1000 for i in forecast_list]

optimal_estimate = sum(x * y for x, y in zip(mae_list, forecast_list))
print(" ")
print('Optimal Weighted Estimate: ', round(optimal_estimate, decimals))    



#%%
# =============================================================================
# 5 - Plot distribution versus Bloomberg
# =============================================================================

def get_month_year_quarter(date_string):
    # Convert the date string to a datetime object
    date_object = datetime.strptime(date_string, '%Y-%m-%d')  # Adjust the format to match your date string
    
    # Extract month and year as strings
    month_string = date_object.strftime('%B')  # Full month name, e.g., "January"
    year_string = date_object.strftime('%Y')   # Full year as a string, e.g., "2023"
    
    # Calculate the quarter
    quarter = (date_object.month - 1) // 3 + 1
    quarter_string = f'Q{quarter}'  # Quarter as a string, e.g., "Q3"
    
    return month_string, year_string, quarter_string

def max_decimal_places_in_list(float_list):  
    
    max_decimal_places = 0

    for num in float_list:
        # Convert the float to a string
        num_str = str(num)

        # Find the position of the decimal point in the string
        decimal_point_index = num_str.find('.')

        if decimal_point_index != -1:
            # Calculate the number of decimal places in the float
            decimal_places = len(num_str) - decimal_point_index - 1

            # Update max_decimal_places if necessary
            max_decimal_places = max(max_decimal_places, decimal_places)

    return max_decimal_places

def format_to_decimal_places(value, decimal_places):
    if isinstance(value, str):
        try:
            value = float(value)
        except ValueError:
            raise ValueError("Input value must be a numerical (int or float) or a string representation of a numerical value.")

    rounded_value = round(value, decimal_places)
    format_string = "{:.%df}" % decimal_places
    formatted_value = format_string.format(rounded_value)
    return formatted_value
    return formatted_value

date_string = forecast_month
release_month, release_year, release_quarter = get_month_year_quarter(date_string)
if freq == 'QS':
    release_month = release_quarter    
release_period = release_month + ' ' + release_year

X = forecast_list
w = mae_list

df_bbg = pd.read_excel(r'N:\Common\MACRO MARKETS\Team\Forecasting\BBG_Input\input_bbg_distribution.xlsx')
country = df_bbg.iloc[2, 2]
release_indicator = df_bbg.iloc[3, 2]
reference_month = df_bbg.iloc[4, 2]
bbg_stdev = df_bbg.iloc[16, 2]
if type(bbg_stdev) == str:
    if bbg_stdev[-1] == 'k':
        bbg_stdev = bbg_stdev[:-1]
bbg_median = df_bbg.iloc[10, 2]
if type(bbg_median) == str:
    if bbg_median[-1] == 'k':
        bbg_median = bbg_median[:-1]
prior = y[-1]
if indicator == 'uslama4979' or indicator == 'uslama1304':
    prior = prior/1000
    
df_bbg = df_bbg[df_bbg['Unnamed: 3'].notna()]
df_bbg = df_bbg.rename(columns=df_bbg.iloc[0]).drop(df_bbg.index[0])
BBG = df_bbg.Estimate.values.reshape(-1)

max_decimals = max_decimal_places_in_list(BBG)
bbg_median = format_to_decimal_places(bbg_median, max_decimals)
optimal_estimate = format_to_decimal_places(optimal_estimate, max_decimals)
prior = format_to_decimal_places(prior, max_decimals)

XandBBG = np.concatenate([forecast_list,BBG])

h = X
bins = np.linspace(math.floor(XandBBG.min())-1,math.ceil(XandBBG.max())+1,10)

plt.style.use('seaborn')
fig, ax = plt.subplots(1, 1, constrained_layout=True, figsize=(5.8, 3.8))

ax2 = ax.twinx()
weighted = sm.nonparametric.KDEUnivariate(X)
weighted.fit(kernel='epa',bw=float(bbg_stdev)*2,fft=False,weights=w)
ax2.plot(weighted.support,weighted.density,label="Density of Model F'casts (rhs)",color='b',linewidth=2.5,linestyle='--',dashes=(5,1))
ax2.fill_between(weighted.support,weighted.density,facecolor='b',alpha=0.2)

weighted_BBG = sm.nonparametric.KDEUnivariate(BBG)
weighted_BBG.fit(fft=False)
ax2.plot(weighted_BBG.support,weighted_BBG.density,label="Density of BBG F'casts (rhs)",color='g',linewidth=2,linestyle='--',dashes=(5,1))
ax2.fill_between(weighted_BBG.support,0,weighted_BBG.density,facecolor='g',alpha=0.2)

ax.set_ylim([0,weighted.density.max()*3])
ax.set_xlim([math.floor(XandBBG.min())-float(bbg_stdev)*2,math.ceil(XandBBG.max())+float(bbg_stdev)*2])
ax2.set_ylim([0,weighted.density.max()*3])
ax2.set_xlim([math.floor(XandBBG.min())-float(bbg_stdev)*2,math.ceil(XandBBG.max())+float(bbg_stdev)*2])

ax.set_ylabel('Density')

ax2.set_ylabel('Density')
ax2.grid(None)

ax.set_title(country + ': ' + release_indicator + ' in '+ release_period)

ax.plot(float(optimal_estimate),0,marker=7,markersize=10,markeredgecolor='k',markerfacecolor='b',markeredgewidth=1,label='Optimal Model Forecast = ' + optimal_estimate,linewidth=0)
# Line to plot actual print:
# ax.plot(46.7,0,marker=7,markersize=10,markeredgecolor='k',markerfacecolor='r',markeredgewidth=1,label='Actual = 46.7',linewidth=0)

ax.plot(float(bbg_median),0,marker=7,markersize=10,markeredgecolor='k',markerfacecolor='g',markeredgewidth=1,label='BBG Consensus = ' + bbg_median,linewidth=0)
ax.plot([], [], ' ', label='BBG Standard Deviation  = ' + str(bbg_stdev))
ax.plot([], [], ' ', label='Prior  = ' + prior)

ax.legend(bbox_to_anchor=(0,1), loc = "upper left", ncol=1, frameon=False)

# Change location of labels
ax.text(0.20,0.5,'Economists\nin BBG',fontsize=9,color='g',horizontalalignment='center',verticalalignment='center',rotation=0,transform=ax.transAxes)
ax.text(0.50,0.40,'Our Models',fontsize=9,color='b',horizontalalignment='left',verticalalignment='center',rotation=0,transform=ax.transAxes)

plt.savefig(r'N:\Common\MACRO MARKETS\Team\Forecasting\Plots\BBG_Distribution.png',dpi=200)
from PIL import Image
img = Image.open(r'N:\Common\MACRO MARKETS\Team\Forecasting\Plots\BBG_Distribution.png')
img.show()







































# Rigobon

import pandas as pd
import numpy as np
import datetime as dt
from statsmodels.tsa.api import VAR
from scipy import signal
from scipy.optimize import minimize
# from xbbg import blp
# import Connectivity.Conn_bbg as bbg

import statsmodels.api as sm

#Set the Table Display options
pd.set_option('display.width', 400)
pd.set_option('display.max_columns', 30)
np.set_printoptions(precision = 4,linewidth = 200)

####################################################
def load_data(rolling_window,slide = 0,file_loc = r"H:\Model\Rigobon\data_d.xlsx" ):
    # Load the Yield Data
    #ticker = [['USGG10YR Index', 'US'],
    #          ['GUKG10 Index', 'UK'],
    #          ['GDBR10 Index', 'GE'],
    #          ['GJGB10 Index', 'JP']
    #          ]

    #start_date = dt.date(2000, 1, 1)
    #end_date = dt.date(2030, 12, 31)
    #field = 'PX LAST'
    #tickers = list(np.array(ticker)[:, 0])
    #cols = list(np.array(ticker)[:, 1])
    #data_raw = bbg.Df_BBG(tickers, field, start_date, end_date, cols) * 100
    data_raw = pd.read_excel(r'N:\Common\MACRO MARKETS\Models\Models\Rates\rigobon_input_data.xlsx', sheet_name = 'input', index_col = 0)
    data_raw = data_raw.fillna(method = 'ffill').dropna()


   # Detrend the Data
    data_regress = pd.DataFrame(data_raw.values, columns = data_raw.columns)

    results = []
    i=0
    for country in data_regress.columns:
        X = data_regress.index
        X = sm.add_constant(X)
        y = data_regress[str(country)]
        model = sm.OLS(y,X).fit()
        results.append(model.params)

        data_raw[country ] = data_raw[country ] -(results[i][0] + results[i][1]*data_regress.index )
        i = i +1

    # Resample on a weekly basis and keep the last available data point
    data_w = data_raw.resample('W-FRI').mean() * 100 # FRI
    data_w.append(data_raw[-1:]*100)

    N = int(rolling_window)
    N_slide = int(slide)
    if N_slide == 0:
        data_w = data_w[-N:]
    else:
        data_w = data_w[-N - N_slide:-N_slide]

    # Detrend the Dataset
    data_dt = data_w

    # Build the difference in the weekly yield
    data = data_dt - data_dt.shift(1)

    # Clean the dataset
    data = data.dropna()

    return data_raw,data_w,data_dt,data

# Build the VaR Model for the reduced form
def var_model(data, lags = 2 ):
    # Build the VAR Model using the following Specification
    model = VAR(data)
    results = model.fit(maxlags=lags)

    # Results from the VAR model
    params = results.coefs
    residual = results.resid
    simulation = pd.DataFrame(results.simulate_var(steps = 10), columns = data.columns).dropna()

    mat_cov =  residual.cov()

    return  residual,params[0],params[1], simulation,mat_cov

# Identify the various vol regimes
def vol_regime(residual,threshold):
    # Normalize the residuals
    residual_norm = residual / residual.std()

    # Regime allocation
    df_regime = residual_norm.applymap(lambda x: (abs(x) > threshold)) * 1
    df_regime['Total'] = df_regime.sum(axis=1)

    N = len(df_regime)

    for i in range(N):
        if (df_regime.iloc[i, :])['Total'] > 1:
            # at least 2 high vol regime at the same time so no interests and put the whole row at a value of zero
            df_regime.iloc[i, :] = 0
        else:
            # Only low regimes (either all of them or only one)
            df_low_vol = residual.loc[df_regime['Total'] == 1]
            df_high_vol_US = residual.loc[df_regime['US'] == 1]
            df_high_vol_UK = residual.loc[df_regime['UK'] == 1]
            df_high_vol_GE = residual.loc[df_regime['GE'] == 1]
            df_high_vol_JP = residual.loc[df_regime['JP'] == 1]



    return df_low_vol, df_high_vol_US, df_high_vol_UK, df_high_vol_GE, df_high_vol_JP
def Cov_regime(*args):
    # args = df_low_vol, df_high_vol_us, df_high_vol_uk, df_high_vol_ge, df_high_vol_jp
    # Compute the Variance Covariance Matrix in each Regime
    cov_matrix_list = []
    for i in range(len(args)):
        # check if enough data points to estimate the covariance matrix
        if len(args[i])>1:
            # Build the covariance matrix
            cov_ = args[i].cov()
            cov_ = np.matrix(cov_)
        else:
            # Replace the covaraince matrix with the identity matrix
            cov_ = np.matrix(np.eye(4))

        cov_matrix_list.append(cov_)

    return cov_matrix_list

# Procedure to flatten and rebuild matrices for the optimisation function
def Mat_to_vec(X):
    return list(np.asarray(X).flatten())
def Mat_agg(X):
    n = len(X)
    out = []
    for i in range(n):
        out.extend(Mat_to_vec(X[i]))
    return np.array(out)
def Vec_to_Mat(X, n):
    return X.reshape(n, n)
def Mat_full(X, n):
    mat = np.zeros(shape=(n, n))
    c = 0

    for i in range(n):
        mat[i][i] = X[i]

    return mat
def Mat_full_cont(X, n):
    mat = np.identity(n)

    mid = int(len(X) / 2)
    c = 0
    for i in range(0, n):
        for j in range(i + 1, n):
            mat[i][j] = X[c]
            mat[j][i] = X[c + mid]
            c = c + 1

    return mat
def Mat_agg_split(X, n):
    x = np.array(X)
    N = int(len(x) / (n * n))

    out = []
    for i in range(N):
        out.append(Vec_to_Mat((x[i * n * n:i * n * n + n * n]), n))

    return out
def cov_to_flat(X):
   out = np.array(X).flatten()
   out = np.block([out[0:4],out[5:8],out[10:12],out[15:16]])
   return out

# Optimization functions
def objective(x,Y):  # Flat vector

    # Rebuild all the matrices
    mat_a = Mat_full_cont(x[0:12], 4)
    mat_x1 = Mat_full(x[12:16], 4).T  # low vol cov
    mat_x2 = Mat_full(x[16:20], 4).T  # us cov
    mat_x3 = Mat_full(x[20:24], 4).T  # uk cov
    mat_x4 = Mat_full(x[24:28], 4).T  # ge cov
    mat_x5 = Mat_full(x[28:32], 4).T  # jp cov

    # Construct the norm
    l = GMM(mat_x1, mat_a, Y[0]) + GMM(mat_x2, mat_a, Y[1]) + GMM(mat_x3, mat_a, Y[2]) + GMM(mat_x4, mat_a, Y[3]) + GMM(mat_x5, mat_a, Y[4])
    return l
def GMM(XX, A, X):
    # Cholesky part to force a symmetric and define positive matrix
    #X_chol = X @ X.T
    #t = (XX - A @ X_chol @ A.T)

    X_chol = (XX) @ XX.T
    t = (X_chol - A @ X @ A.T)
    norm = np.multiply(t, t).sum()

    return norm
def estimate(cov_regime_list):
    ###### initial random matrix using the identity matrix
    A = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] # A is matrix 4x3 with 4 Yields, one current and 2 lags
    X1 = [1, 1, 1, 1]
    X2 = [1, 1, 1, 1]
    X3 = [1, 1, 1, 1]
    X4 = [1, 1, 1, 1]
    X5 = [1, 1, 1, 1]

    # Flatten all the inputs into a single flat vector
    x0 = (Mat_agg([A, X1, X2, X3, X4, X5]))

    #Minimize the objective function
    Z = minimize(objective, x0, cov_regime_list, options={'disp': False},method = 'L-BFGS-B') # ,  #SLSQP / BFGS / TNC / L-BFGS-B

    # Rebuild all the matrices
    xf = Z.x
    mat_a = Mat_full_cont(xf[0:12], 4)

    # mat_x1 = pd.DataFrame(Mat_full(xf[12:22], 4).T)
    # mat_x2 = Mat_full(xf[22:32], 4)
    # mat_x3 = Mat_full(xf[32:42], 4)
    # mat_x4 = Mat_full(xf[42:52], 4)
    # mat_x5 = Mat_full(xf[52:62], 4)

    mat_A = pd.DataFrame(mat_a, index=['US', 'UK', 'GE', 'JP'], columns=['US', 'UK', 'GE', 'JP'])
    return mat_A

# Rigobon Analysis
def rigobon(threshold, rolling_window, steps,slide = 0):

    # # Load the data
    data_raw, data_w, data_dt, data = load_data(rolling_window,slide)

    # print(data)
    # input('here')

    # Build the VAR model
    residual, param1, param2, simulation, mat_cov = var_model(data)
    df_low_vol, df_high_vol_US, df_high_vol_UK, df_high_vol_GE, df_high_vol_JP = vol_regime(residual, threshold)

    # Build the covariance regimes
    Y = Cov_regime(df_low_vol, df_high_vol_US, df_high_vol_UK, df_high_vol_GE, df_high_vol_JP)

    # Estimate the Matrix A based on the Rigobon identification
    mat_A = estimate(Y)

    # Residual covariance matrix decomposition
    P = np.linalg.cholesky(mat_cov)

    # forecasting steps for the decomposition
    n_steps = int(steps)
    n = len(param1)

    # Companion Matrix
    A = np.block([[param1, param2],
                  [np.identity(n), np.zeros((n, n))]
                  ])

    # Compute phi(i)
    phi_i = {}
    phi_i[0] = np.identity(len(A))
    for i in range(1, n_steps):
        phi_i[i] = phi_i[i - 1] @ A

    for i in range(n_steps):
        phi_i[i] = phi_i[i][:n, :n]

    # Compute theta(i)
    theta_i = {}
    for i in range(n_steps):
        theta_i[i] = phi_i[i] @ P

    # Compute the total variance elements
    var = np.zeros((n, n))
    for i in range(n_steps):
        var = var + theta_i[i] @ (theta_i[i].T)


    # Compute the individual variance points
    omega = np.zeros((n, n))

    for j in range(n):
        for k in range(n):
            for i in range(n_steps):
                omega[k, j] = omega[k, j] + (theta_i[i][k, j]) * (theta_i[i][k, j])

    omega_original = var #mat_a @ omega @ (mat_a.T)

    # Compute the Normalized variance matrix
    omega_norm = omega_original.copy()
    norm = np.array(omega_norm.sum(axis=1))
    for j in range(n):
        for k in range(n):
            omega_norm[j, k] = omega_norm[j, k]  / norm[j]


    # Spillover index
    spillover = (np.sum(omega) - np.trace(omega))/np.sum(omega)

    #Cumulative residual of the structural forms over the last 3 months
    df_residuals =  (mat_A @ residual.T).T

    return data, mat_A, pd.DataFrame(omega*100, index = data.columns, columns=data.columns), spillover, df_residuals
def rolling_rigobon(threshold, rolling_window, steps, N = 20):
    print('   ')
    print('Run the Rigobon model....')

    spill = []
    mat_a = []
    date = []
    residual =[]

    for i in range(N+1):
        print('___Iteration # '+str(i)+' / '+str(N+1))
        data, mat_A, omega , spillover, df_residuals = rigobon(threshold, rolling_window, steps,N-i)
        spill.append(spillover)
        mat_a.append(np.diag(np.linalg.inv(mat_A)))
        date.append(data.index[-1])
        residual.append((df_residuals[-1:].values)[0])

    df_spill = pd.DataFrame(spill, columns= ['Spillover'], index = date)
    df_mat_a = pd.DataFrame(mat_a,columns = data.columns,index = date)
    df_residual = pd.DataFrame(residual,index = date)
    print('Model Completed....')
    print('     ')
    return pd.concat([df_spill,df_mat_a,df_residual], join='outer', axis = 1)

#################################################
# Main Model
#################################################

def rigobon_output():
    threshold = 1.6
    rolling_window = 100
    steps = 5
    N = 20

    # Load the historical saved data
    data_hist = pd.read_excel(r"N:\Common\MACRO MARKETS\Models\Models\Rates\rigobon_data.xlsx", index_col=0)

    # Run the rigobon on the last N period required
    today = dt.datetime.today()
    last_update = data_hist.index[-1:][0]
    n_days = (today - last_update).days
    n_week = int(n_days/7)

    new_data = rolling_rigobon(threshold, rolling_window, steps,n_week)

    # Merge the tables
    data_output = pd.concat([data_hist,new_data], axis = 0 , join = 'outer')

    # Write the updated table
    #data_output.to_excel(r"N:\Common\MACRO MARKETS\Models\Models\Rates\rigobon_data.xlsx")
    data_output.columns = ['Spillover','mUS','mUK','mGE','mJP','rUS','rUK','rGE','rJP']

    # Shock standardised
    # data_output['US'] = (data_output['rUS'] / data_output['rUS'][-N:].std())
    # data_output['UK'] = (data_output['rUK'] / data_output['rUK'][-N:].std())
    # data_output['GE'] = (data_output['rGE'] / data_output['rGE'][-N:].std())
    # data_output['JP'] = (data_output['rJP'] / data_output['rJP'][-N:].std())
    data_output['US'] = (data_output['rUS'] / data_output['rUS'].std())
    data_output['UK'] = (data_output['rUK'] / data_output['rUK'].std())
    data_output['GE'] = (data_output['rGE'] / data_output['rGE'].std())
    data_output['JP'] = (data_output['rJP'] / data_output['rJP'].std())

    # Cumulative shocks last N days
    data_shock = data_output[-N:].cumsum(axis=0)
    for x in data_shock.columns:
        data_shock[x] = data_shock[x] - data_shock[x][0]

    return data_output,data_shock


threshold = 1.6
rolling_window = 100
steps = 5
N = 20

# x = rigobon(threshold, rolling_window,steps,0)
# print(x[1])
df_model_spillover,df_model_shock = rigobon_output()
# df_model_spillover.plot()
df_model_spillover.to_excel(r'N:\Common\MACRO MARKETS\Models\Models\Rates\Rigobon_Output_Test.xlsx')





















# Energy CPI RFA

#%%
# =============================================================================
# 1 - Import modules
# =============================================================================

from matplotlib import pyplot as plt
import pandas as pd
import numpy as np
from scipy import stats
from statsmodels.tsa.api import VAR
import statsmodels.api as sm
from datetime import datetime
from datetime import timedelta
from collections import OrderedDict
import matplotlib
matplotlib.use('agg')
from scipy.optimize import differential_evolution, LinearConstraint
from scipy.optimize import minimize_scalar
pd.options.mode.chained_assignment = None  # default='warn'
import seaborn as sns
from dateutil.relativedelta import relativedelta
plt.style.use("seaborn-darkgrid")
sns.set_palette("tab10")
palette = sns.color_palette()
import pandas as pd
import numpy as np
from dateutil.relativedelta import relativedelta
import win32com.client
import time
from datetime import date
from sklearn.preprocessing import StandardScaler
import statsmodels.api as sm
import time
from sklearn.linear_model import LinearRegression
pd.options.mode.chained_assignment = None  # default='warn'
from statsmodels.tsa.seasonal import seasonal_decompose
import json
import datetime

def getDataframe(db, unifiedSeriesRequest):
    series = db.FetchSeries(unifiedSeriesRequest)
    return pd.DataFrame({s.Name: toPandasSeries(s) for s in series})

def toPandasSeries(series):
    # For some reason, pandas 0.19 does not like the result when to_datetime
    # is used directly on the array DatesAtStartOfPeriod. Convert to string first.
    pdates = pd.to_datetime([d.strftime('%Y-%m-%d') for d in series.DatesAtStartOfPeriod])
    return pd.Series(series.values, index=pdates)


#%%
# =============================================================================
# 2 - Loading and processing data
# =============================================================================

forecast_month = '2024-06-01'
MB_start_month = '2016-01-01' # 

print('Fetching Macrobond Data...')

df_input = pd.read_excel(r'N:\Common\MACRO MARKETS\Team\Forecasting\CPI\Energy\Energy_Input.xlsx', sheet_name = "Energy_Index_NSA")

c = win32com.client.Dispatch("Macrobond.Connection")
d = c.Database
r = d.CreateUnifiedSeriesRequest()

Energy = pd.DataFrame(index = pd.date_range(MB_start_month , date.today().strftime("%Y-%m-%d"), freq= "MS"))

for i in range(len(df_input)):
    r = d.CreateUnifiedSeriesRequest()
    r.AddSeries(df_input.iloc[i, 0])
    df_temp = getDataframe(d, r)
    df_temp = df_temp.pct_change()*100
    df_temp = df_temp.dropna()
    Energy = Energy.join(df_temp, how = 'left')

Energy_ = Energy.dropna()

print("Energy Input Data Loaded.")



df_input = pd.read_excel(r'N:\Common\MACRO MARKETS\Team\Forecasting\CPI\Energy\Energy_Input.xlsx', sheet_name = "Energy_Weights")

c = win32com.client.Dispatch("Macrobond.Connection")
d = c.Database
r = d.CreateUnifiedSeriesRequest()

Energy_Weights = pd.DataFrame(index = pd.date_range(MB_start_month , date.today().strftime("%Y-%m-%d"), freq= "MS"))

for i in range(len(df_input)):
    r = d.CreateUnifiedSeriesRequest()
    r.AddSeries(df_input.iloc[i, 0])
    df_temp = getDataframe(d, r)
    df_temp = df_temp.dropna()
    Energy_Weights = Energy_Weights.join(df_temp, how = 'left')

Energy_Weights_ = Energy_Weights.dropna()

print("Energy Weight Data Loaded.")


df_input = pd.read_excel(r'N:\Common\MACRO MARKETS\Team\Forecasting\CPI\Energy\Energy_Input.xlsx', sheet_name = "Regressors")

c = win32com.client.Dispatch("Macrobond.Connection")
d = c.Database
r = d.CreateUnifiedSeriesRequest()

Regressors = pd.DataFrame(index = pd.date_range(MB_start_month , date.today().strftime("%Y-%m-%d"), freq= "MS"))

for i in range(len(df_input)):
    print(df_input['Ticker'][i])
    r = d.CreateUnifiedSeriesRequest()
    r.AddSeries(df_input.iloc[i, 0])
    df_temp = getDataframe(d, r)
    if df_input['Ticker'][i] == "uscaes0310":
        today = pd.to_datetime(datetime.date.today())
        last_date = df_temp.index.max()
        missing_dates = pd.date_range(start=last_date + pd.DateOffset(days=1), end=today)
        missing_data = pd.DataFrame({'uscaes0310': [None] * len(missing_dates)}, index=missing_dates)
        df_temp = pd.concat([df_temp, missing_data])
    if i == 0:
        daily_data = df_temp.rolling(window = 30).mean()
        df_temp = daily_data.resample('MS').last()
        df_temp = df_temp.pct_change()*100
        df_temp = df_temp.dropna()
        #result = seasonal_decompose(df_temp, model='additive', period=12)
        #x = np.array(df_temp.values).flatten() - result.seasonal.values
        #df_sa = pd.Series(data=x, index=df_temp.index, name=df_input.iloc[i, 1] )
        Regressors = Regressors.join(df_temp, how = 'left')
    else: 
        df_temp = df_temp.resample('MS').mean()
        df_temp = df_temp.pct_change()*100
        df_temp = df_temp.dropna()
        #result = seasonal_decompose(df_temp, model='additive', period=12)
        #x = np.array(df_temp.values).flatten() - result.seasonal.values
        #df_sa = pd.Series(data=x, index=df_temp.index, name=df_input.iloc[i, 1] )
        Regressors = Regressors.join(df_temp, how = 'left')

Regressors_ = Regressors.dropna()

print("Regressor Data Loaded.")


print(" ")
print("All Data Loaded.")

#%%
# =============================================================================
# 3 - Prepare the Dataframe
# =============================================================================

def rfa_regression(X, y, X_forecast_month, components, print_regression, stopping_threshold, min_r2_threshold):
    estimator = LinearRegression()
    selected_features = []
    remaining_features = list(X.columns)
    best_score = -float('inf')
    
    for _ in range(components):
        best_feature = None
        
        for feature in remaining_features:
            current_features = selected_features + [feature]
            X1 = X[current_features]
            X1['constant'] = 1
            X1 = X1.join(y, how='right')
            X1 = X1.iloc[:, :-1]
            
            reg = sm.OLS(endog=y, exog=X1, missing='drop')
            results = reg.fit()
            
            if results.rsquared_adj > best_score:
                best_score = results.rsquared_adj
                best_feature = feature
        
        if best_feature == None:
            break 
        
        else:
            selected_features.append(best_feature)
            remaining_features.remove(best_feature)
        if best_score > min_r2_threshold or best_score - results.rsquared_adj < stopping_threshold:
            break
    
    X1 = X[selected_features]
    X_forecast_month = X_forecast_month[selected_features]
    X1['constant'] = 1
    X_forecast_month['constant'] = 1
    
    X1 = X1.join(y, how='right')
    X1 = X1.iloc[:, :-1]
    
    reg = sm.OLS(endog=y, exog=X1, missing='drop')
    rfa_results = reg.fit()
    
    if print_regression:
        print(" ")
        print(rfa_results.summary2())
    
    rfa_forecast = np.dot(rfa_results.params, X_forecast_month)
    
    return rfa_forecast, rfa_results.rsquared_adj, rfa_results.params, X1.columns, rfa_results.fittedvalues

Energy = Energy_
Energy_Weights = Energy_Weights_
Regressors = Regressors_

standardise = False
lags = 1
components = 3
stopping_threshold = 0.01
min_r2_threshold = 0.99
Energy_Prices_length = len(Energy.columns)
Regressors_length = len(Regressors.columns)


Energy_Prices_lags = Energy.copy()
Energy_Prices_lags = Energy_Prices_lags.join(Regressors, how = 'right')
Energy_Prices_lags = Energy_Prices_lags.iloc[:, :Energy_Prices_length]
for i in range(lags):
    for ii in Energy_Prices_lags.columns[:Energy_Prices_length]:
        Energy_Prices_lags[ii + '_lag_' + str(i+1)] = Energy_Prices_lags[ii].shift(i+1)

Energy_Prices_lags = Energy_Prices_lags.iloc[:, Energy_Prices_length:]


Regressor_lags = Regressors.copy()
for i in range(lags):
    for ii in Regressor_lags.columns:
        Regressor_lags[ii + '_lag_' + str(i+1)] = Regressor_lags[ii].shift(i+1)

Regressor_lags = Regressor_lags.iloc[:, Regressors_length:]


df_reg = Regressors.copy()
df_reg = df_reg.join(Energy_Prices_lags, how = 'inner')
df_reg = df_reg.join(Regressor_lags, how = 'inner')
# df_reg = df_reg.join(Regressor_lags, how = 'left')
# df_reg = df_reg.iloc[lags:, :]
df_reg = df_reg.dropna()


#%%
# =============================================================================
# 4 - Run the feature selection model
# =============================================================================

component_forecasts_rfa = []
component_rsq_rfa = []
rfa_params = []
rfa_regressors = []

for i in range(len(Energy.columns)):
    print("Component Number: ", i+1)
    
    indicator = Energy.columns[i]
    
    X = df_reg.copy()
    
    X_forecast_month = X.iloc[-1, :]
    X = X.iloc[:-1, :]
    
    X = X.join(Energy[indicator], how = 'inner')
    X = X.dropna()

    y = X[indicator]
    X = X.drop(indicator, axis = 1)
    
    rfa_forecast, rfa_results_rsquared_adj, rfa_param, rfa_regressor, fitted_values = rfa_regression(X, y, X_forecast_month, components, True, stopping_threshold, min_r2_threshold)
    component_forecasts_rfa.append(rfa_forecast)
    component_rsq_rfa.append(rfa_results_rsquared_adj)
    rfa_params.append(rfa_param)
    rfa_regressors.append(rfa_regressor)
    print(fitted_values[-12:])

print(" ")
print("Average R-Squared RFA: ", round(np.sum(component_rsq_rfa)/len(component_rsq_rfa), 3))
print("NSA Energy Forecast: ", round(np.dot(component_forecasts_rfa, Energy_Weights.iloc[-1, :].values/Energy_Weights.sum(axis=1)[-1]), 3))



#%%
# =============================================================================
# 5 - Seasonal Adjustment
# =============================================================================


MB_start_month = '2016-01-01'

print('Fetching Macrobond Data...')

df_input = pd.read_excel(r'N:\Common\MACRO MARKETS\Team\Forecasting\CPI\Energy\Energy_Input.xlsx', sheet_name = "Energy_Index_NSA")

c = win32com.client.Dispatch("Macrobond.Connection")
d = c.Database
r = d.CreateUnifiedSeriesRequest()

Energy = pd.DataFrame(index = pd.date_range(MB_start_month , date.today().strftime("%Y-%m-%d"), freq= "MS"))

for i in range(len(df_input)):
    r = d.CreateUnifiedSeriesRequest()
    r.AddSeries(df_input.iloc[i, 0])
    df_temp = getDataframe(d, r)
    df_temp = df_temp.dropna()
    Energy = Energy.join(df_temp, how = 'left')

Energy = Energy.dropna()

print("Energy Input Data Loaded.")

MB_start_month = '2016-01-01'


df_input = pd.read_excel(r'N:\Common\MACRO MARKETS\Team\Forecasting\CPI\Energy\Energy_Input.xlsx', sheet_name = "Energy_Index")

c = win32com.client.Dispatch("Macrobond.Connection")
d = c.Database
r = d.CreateUnifiedSeriesRequest()

Energy_SA = pd.DataFrame(index = pd.date_range(MB_start_month , date.today().strftime("%Y-%m-%d"), freq= "MS"))

for i in range(len(df_input)):
    r = d.CreateUnifiedSeriesRequest()
    r.AddSeries(df_input.iloc[i, 0])
    df_temp = getDataframe(d, r)
    df_temp = df_temp.dropna()
    Energy_SA = Energy_SA.join(df_temp, how = 'left')

Energy_SA = Energy_SA.dropna()

print("Energy SA Input Data Loaded.")

Energy = Energy.join(Energy_SA, how = 'inner')

# seasonal adjustment factor = NSA/SA

Energy['SA1'] = Energy.iloc[:, 0]/Energy.iloc[:, 6]
Energy['SA2'] = Energy.iloc[:, 1]/Energy.iloc[:, 7]
Energy['SA3'] = Energy.iloc[:, 2]/Energy.iloc[:, 8]
Energy['SA4'] = Energy.iloc[:, 3]/Energy.iloc[:, 9] 
Energy['SA5'] = Energy.iloc[:, 4]/Energy.iloc[:, 10]
Energy['SA6'] = Energy.iloc[:, 5]/Energy.iloc[:, 11]

nsa_forecasts = []
sa_forecasts = []
sa_mom_forecasts = []

for i in range(len(component_forecasts_rfa)):
    new_nsa = Energy.iloc[-1, i]*(1+component_forecasts_rfa[i]/100)
    nsa_forecasts.append(new_nsa)
    new_sa = new_nsa/Energy.iloc[-12, -6+i]
    sa_forecasts.append(new_sa)
    sa_mom_forecast = (new_sa - Energy.iloc[-1, 6+i])/Energy.iloc[-1, 6+i]
    sa_mom_forecasts.append(sa_mom_forecast*100)

print(" ")
print(" ")
print("Final Energy SA Forecast: ", round(np.dot(sa_mom_forecasts, Energy_Weights.iloc[-1, :].values/Energy_Weights.sum(axis=1)[-1]), 3))
print(" ")
print(" ") 


#%%
# =============================================================================
# 6 - Backtesting MAE
# =============================================================================
"""
df_bt = df_reg.copy()
df_bt['constant'] = 1

# Set i as how many months ago you want to check regression result, i = 1 is the most recent release
i = 6

backtest_obs = []
actual_obs = [0.57, 3.34, 0.34, 1.44, -1.25, 1.53]

for iiii in range(i):
        
    
    component_forecasts_bt = []
    
    for ii in range(len(rfa_params)):
        count = 0.0
        for iii in range(len(rfa_params[ii])):
            count += df_bt[rfa_regressors[ii][iii]][-iiii-2]*rfa_params[ii][iii]
        component_forecasts_bt.append(count)
    backtest_obs.append(round(np.dot(component_forecasts_bt, Energy_Weights.iloc[-iiii-2, :].values/Energy_Weights.sum(axis=1)[-iiii-2]), 3))
        
diff = []

for ii in range(i):
    diff.append(abs(backtest_obs[ii]-actual_obs[ii]))

print(" ")
print("Energy Inflation BT: ", round(np.dot(component_forecasts_bt, Energy_Weights.iloc[-1-i, :].values/Energy_Weights.sum(axis=1)[-1-i]), 3))
print(" ")
print(component_forecasts_bt)
print(" ")
print("Six month MAE: ", sum(diff))
"""


#%%
# =============================================================================
# Backtesting by Month
# =============================================================================

df_bt = df_reg.copy()
df_bt['constant'] = 1

# Set i as how many months ago you want to check regression result, i = 1 is the most recent release
i = 0
component_forecasts_bt = []

for ii in range(len(rfa_params)):
    count = 0.0
    for iii in range(len(rfa_params[ii])):
        count += df_bt[rfa_regressors[ii][iii]][-i-1]*rfa_params[ii][iii]
    component_forecasts_bt.append(count)

print(" ")
print("Energy Inflation BT: ", round(np.dot(component_forecasts_bt, Energy_Weights.iloc[-1-i, :].values/Energy_Weights.sum(axis=1)[-1-i]), 3))
print(" ")
print(component_forecasts_bt)


































# Rents/OER Arima
#%%
# =============================================================================
# 1 - Import modules
# =============================================================================

from matplotlib import pyplot as plt
import pandas as pd
import numpy as np
from scipy import stats
from statsmodels.tsa.api import VAR
import statsmodels.api as sm
from datetime import datetime
from datetime import timedelta
from collections import OrderedDict
from PIL import Image
import matplotlib
matplotlib.use('agg')
from scipy.optimize import differential_evolution, LinearConstraint
from scipy.optimize import minimize_scalar
pd.options.mode.chained_assignment = None  # default='warn'
import seaborn as sns
from dateutil.relativedelta import relativedelta
plt.style.use("seaborn-darkgrid")
sns.set_palette("tab10")
palette = sns.color_palette()
from statsmodels.tsa.arima.model import ARIMA
import pandas as pd
import numpy as np
from dateutil.relativedelta import relativedelta
import win32com.client
import time
from datetime import date

import statsmodels.api as sm
from sklearn.linear_model import Lasso
import time
pd.options.mode.chained_assignment = None  # default='warn'
from statsmodels.tsa.seasonal import seasonal_decompose

def getDataframe(db, unifiedSeriesRequest):
    series = db.FetchSeries(unifiedSeriesRequest)
    return pd.DataFrame({s.Name: toPandasSeries(s) for s in series})

def toPandasSeries(series):
    # For some reason, pandas 0.19 does not like the result when to_datetime
    # is used directly on the array DatesAtStartOfPeriod. Convert to string first.
    pdates = pd.to_datetime([d.strftime('%Y-%m-%d') for d in series.DatesAtStartOfPeriod])
    return pd.Series(series.values, index=pdates)

#%%
# =============================================================================
# 2 -Loading and processing data
# =============================================================================

forecast_month = '2024-06-01'
MB_start_month = '1990-01-01'

print('Fetching Macrobond Data...')

c = win32com.client.Dispatch("Macrobond.Connection")
d = c.Database
r = d.CreateUnifiedSeriesRequest()

rents = pd.DataFrame(index = pd.date_range(MB_start_month , date.today().strftime("%Y-%m-%d"), freq= "MS"))

r = d.CreateUnifiedSeriesRequest()
r.AddSeries("uspric1138")
r.AddSeries("uspric1134")
df_temp = getDataframe(d, r)
df_temp = df_temp.pct_change()*100
df_temp = df_temp.dropna()
rents = rents.join(df_temp, how = 'left')
rents = rents.dropna()

rents.columns = ['OER', 'Rents']

p, d, q = 6, 0, 1

# Fit the ARIMA model
model = ARIMA(rents['OER'], order=(p, d, q))
fit_model = model.fit()

# Forecast the next data point
forecast_OER = fit_model.forecast(steps=1)

# Print the forecasted value
print("Forecasted Value OER:", round(forecast_OER[0], 3))

p, d, q = 6, 0, 1

# Fit the ARIMA model
model = ARIMA(rents['Rents'], order=(p, d, q))
fit_model = model.fit()

# Forecast the next data point
forecast_Rents = fit_model.forecast(steps=1)

# Print the forecasted value
print("Forecasted Value Rents:", round(forecast_Rents[0], 3))
